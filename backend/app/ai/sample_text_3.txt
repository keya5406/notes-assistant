Data Structures and Algorithms - Academic Notes

Chapter 1: Introduction to Data Structures

What is a Data Structure?

A data structure is a particular way of organizing and storing data in a computer so that it can be accessed and modified efficiently. It defines the relationship between data and the operations that can be performed on them. The choice of data structure can significantly affect the performance of an algorithm or application.

There are many types of data structures, broadly categorized as:

- Primitive Data Structures: Basic types such as integers, floats, characters.
- Non-Primitive Data Structures: Arrays, linked lists, stacks, queues, trees, graphs, hash tables.

Data structures provide a means to manage large amounts of data, such as large databases and internet indexing services.

Why Use Data Structures?

Using the right data structure is important because:

- It helps organize data logically.
- It optimizes the efficiency of data operations like search, insertion, deletion, and traversal.
- It affects the speed and resource consumption of software.
- Complex data structures enable the implementation of advanced algorithms.

Chapter 2: Arrays

Definition

An array is a collection of elements identified by index or key, where all elements are of the same data type. Elements are stored in contiguous memory locations which allows constant-time access using indices.

Declaration and Initialization

In many programming languages, arrays are declared with a fixed size:

For example, in C:
```c
int numbers[5]; // declares an array of 5 integers
Chapter 3: Linked Lists

Definition

A linked list is a linear data structure where each element, called a node, contains two parts:

- Data: The actual value stored.
- Pointer (or Reference): A link to the next node in the sequence.

Unlike arrays, linked lists do not store elements in contiguous memory locations. Instead, each node points to the next node, allowing dynamic memory allocation.

Types of Linked Lists

1. Singly Linked List
- Each node points to the next node.
- Traversal is only possible in one direction (forward).
- The last node points to NULL, indicating the end of the list.

2. Doubly Linked List
- Each node has two pointers: one to the next node and one to the previous node.
- Allows traversal in both directions (forward and backward).
- Requires extra memory for the previous pointer.

3. Circular Linked List
- The last node points back to the first node, forming a circle.
- Can be singly or doubly linked.
- Useful for applications needing cyclic traversal.

Advantages of Linked Lists

- Dynamic size: can easily grow or shrink during execution.
- Efficient insertion and deletion at any position (O(1) if node reference is known).
- No need to know the size beforehand.

Disadvantages of Linked Lists

- Sequential access only (no direct access by index).
- Extra memory overhead for storing pointers.
- Poor cache locality compared to arrays due to non-contiguous storage.

Basic Operations

1. Traversal
- Starting from the head node, visit each node until NULL is reached.

2. Insertion
- At the beginning: create a new node, point it to current head, update head.
- At the end: traverse to last node, point last node to new node.
- At a given position: adjust pointers to insert node in between.

3. Deletion
- At the beginning: update head to the next node, free old head.
- At the end: traverse to second last node, set its next pointer to NULL.
- At a given position: adjust pointers to remove node.

Applications

- Implementing stacks and queues.
- Dynamic memory allocation.
- Undo functionality in editors.
- Representing graphs via adjacency lists.

Memory Considerations

Each node requires extra memory for storing pointers, which can add up in large lists. However, the flexibility often outweighs this cost.

---
Chapter 4: Stacks

Definition

A stack is a linear data structure that follows the Last In First Out (LIFO) principle. The last element added to the stack is the first one to be removed.

Basic Operations

1. Push
- Adds an element to the top of the stack.

2. Pop
- Removes the element from the top of the stack.

3. Peek (or Top)
- Returns the top element without removing it.

4. IsEmpty
- Checks if the stack is empty.

Implementation

Stacks can be implemented using arrays or linked lists.

Applications

- Expression evaluation and syntax parsing.
- Backtracking algorithms such as maze solving.
- Undo mechanisms in software applications.
- Function call management in recursion (call stack).

Advantages

- Simple and efficient.
- Supports controlled access to data.

Disadvantages

- Limited access — only the top element is accessible.

---

Chapter 5: Queues

Definition

A queue is a linear data structure that follows the First In First Out (FIFO) principle. The first element added to the queue is the first one to be removed.

Basic Operations

1. Enqueue
- Adds an element to the rear (end) of the queue.

2. Dequeue
- Removes an element from the front of the queue.

3. Front (or Peek)
- Returns the front element without removing it.

4. IsEmpty
- Checks if the queue is empty.

Types of Queues

1. Simple Queue
- Linear queue with fixed front and rear pointers.

2. Circular Queue
- Rear connects back to front to utilize empty spaces efficiently.

3. Priority Queue
- Each element has a priority, and elements with higher priority are dequeued before others.

4. Deque (Double-ended queue)
- Insertion and deletion are allowed at both ends.

Implementation

Queues can be implemented using arrays, linked lists, or circular buffers.

Applications

- CPU scheduling algorithms.
- Data buffering (e.g., keyboard buffers).
- Breadth-First Search (BFS) in graph algorithms.
- Handling requests in web servers.

Advantages

- Efficient for managing data streams and asynchronous data.

Disadvantages

- Limited access — only front and rear elements are accessible.

---

Chapter 6: Trees

Definition

A tree is a hierarchical data structure consisting of nodes connected by edges. It starts from a root node and branches out into child nodes, forming a parent-child relationship.

Key Terminology

- Root: The topmost node of the tree.
- Parent: A node with one or more child nodes.
- Child: A node directly connected to another node when moving away from the root.
- Leaf (or External Node): A node with no children.
- Internal Node: A node with at least one child.
- Sibling: Nodes that share the same parent.
- Edge: Connection between two nodes.
- Path: A sequence of nodes and edges connecting a node with a descendant.
- Depth: Number of edges from root to the node.
- Height: Number of edges on the longest path from the node to a leaf.

Types of Trees

1. Binary Tree
- Each node has at most two children, referred to as the left and right child.

2. Binary Search Tree (BST)
- A binary tree where the left child's value is less than the parent node, and the right child's value is greater.
- Supports efficient searching, insertion, and deletion.

3. AVL Tree
- A self-balancing BST that maintains balance factors to keep tree height logarithmic.
- Rotations are used to maintain balance after insertions and deletions.

4. B-Trees
- Generalization of BST optimized for systems that read and write large blocks of data.
- Nodes can have more than two children.
- Used in databases and file systems.

Tree Traversal Methods

1. In-order Traversal
- Visit left subtree, root node, then right subtree.
- For BSTs, results in sorted order.

2. Pre-order Traversal
- Visit root node, left subtree, then right subtree.
- Useful for copying trees.

3. Post-order Traversal
- Visit left subtree, right subtree, then root node.
- Useful for deleting trees.

4. Level-order Traversal
- Visit nodes level by level from the root down.
- Implemented using queues.

Applications

- Hierarchical data representation (file systems, organizational structures).
- Expression parsing and evaluation.
- Routing algorithms.
- Databases indexing.

Advantages

- Efficient hierarchical data representation.
- Enables fast search, insert, and delete operations (especially BSTs).

Disadvantages

- Complex implementation compared to linear data structures.
- Performance depends on tree balance; unbalanced trees degrade to linked lists.

---

Chapter 7: Graphs

Definition

A graph is a collection of vertices (also called nodes) and edges connecting pairs of vertices. Graphs are used to represent networks, such as social networks, computer networks, and transportation systems.

Key Terminology

- Vertex (Node): Fundamental unit of a graph.
- Edge: Connection between two vertices.
- Directed Graph (Digraph): Edges have a direction (from one vertex to another).
- Undirected Graph: Edges have no direction; connection is bidirectional.
- Weighted Graph: Edges have weights or costs associated with them.
- Unweighted Graph: Edges have no weights.
- Degree: Number of edges connected to a vertex (in-degree and out-degree in directed graphs).
- Path: Sequence of edges connecting vertices.
- Cycle: A path where the first and last vertices are the same.
- Connected Graph: There is a path between every pair of vertices.
- Disconnected Graph: Some vertices are not reachable from others.

Graph Representations

1. Adjacency Matrix
- A 2D array where cell (i,j) indicates the presence (and possibly weight) of an edge between vertices i and j.
- Easy to implement but space-inefficient for sparse graphs (O(V^2)).

2. Adjacency List
- Each vertex maintains a list of adjacent vertices.
- Space-efficient for sparse graphs.
- Commonly used in graph algorithms.

Graph Traversal Algorithms

1. Depth-First Search (DFS)
- Explores as far as possible along each branch before backtracking.
- Uses a stack (often implicit via recursion).
- Useful for pathfinding, cycle detection, topological sorting.

2. Breadth-First Search (BFS)
- Explores all neighbors of a vertex before moving to the next level.
- Uses a queue.
- Useful for shortest path in unweighted graphs, level-order traversal.

Applications of Graphs

- Social network analysis (finding connections and communities).
- Routing algorithms (internet data routing, GPS navigation).
- Scheduling problems (task ordering using topological sort).
- Network flow and matching problems.
- Dependency resolution in software builds.

Graph Algorithms

- Dijkstra’s Algorithm: Finds the shortest path in weighted graphs with non-negative weights.
- Bellman-Ford Algorithm: Finds shortest paths even with negative weights.
- Kruskal’s and Prim’s Algorithms: Find Minimum Spanning Trees.
- Floyd-Warshall Algorithm: Finds shortest paths between all pairs of vertices.

Challenges in Graphs

- Handling cycles and disconnected components.
- Managing large-scale graphs efficiently.
- Choosing appropriate data structures for representation based on graph density.

Chapter 8: Sorting Algorithms

Introduction

Sorting algorithms arrange the elements of a list or array in a certain order — usually ascending or descending. Sorting is fundamental in computer science because it improves the efficiency of other algorithms such as searching and merging.

Types of Sorting Algorithms

1. Bubble Sort

Description:
Bubble Sort is one of the simplest sorting algorithms. It works by repeatedly stepping through the list, comparing adjacent pairs, and swapping them if they are in the wrong order. This process repeats until no swaps are needed, indicating that the list is sorted.

Algorithm Steps:
- Compare adjacent elements.
- Swap if they are in the wrong order.
- Repeat for all elements until sorted.

Time Complexity:
- Worst-case: O(n²)
- Average-case: O(n²)
- Best-case (already sorted): O(n)

Space Complexity:
- O(1) (in-place sorting)

Advantages:
- Simple to implement.
- Suitable for small datasets.

Disadvantages:
- Very inefficient for large datasets.

---

2. Insertion Sort

Description:
Insertion Sort builds the sorted array one element at a time by comparing each new element to the already sorted elements and inserting it at the correct position.

Algorithm Steps:
- Start with the second element.
- Compare it with elements before it.
- Shift all larger elements one position ahead.
- Insert the element at the correct position.
- Repeat for all elements.

Time Complexity:
- Worst-case: O(n²)
- Average-case: O(n²)
- Best-case (already sorted): O(n)

Space Complexity:
- O(1) (in-place sorting)

Advantages:
- Efficient for small or nearly sorted datasets.
- Stable sort (preserves relative order of equal elements).

Disadvantages:
- Inefficient for large datasets.

---

3. Merge Sort

Description:
Merge Sort is a divide and conquer algorithm. It divides the list into two halves, recursively sorts each half, and then merges the sorted halves to produce the final sorted list.

Algorithm Steps:
- Divide the list into halves until each sublist contains one element.
- Merge sublists to produce sorted sublists.
- Repeat merging until the entire list is sorted.

Time Complexity:
- Worst-case: O(n log n)
- Average-case: O(n log n)
- Best-case: O(n log n)

Space Complexity:
- O(n) (requires additional space for merging)

Advantages:
- Efficient and predictable performance.
- Stable sort.
- Works well on large datasets.

Disadvantages:
- Requires extra space.
- Recursive implementation may lead to stack overflow in very large datasets.

---

4. Quick Sort

Description:
Quick Sort is a divide and conquer algorithm that selects a pivot element and partitions the list into two sublists: elements less than the pivot and elements greater than the pivot. It recursively sorts the sublists.

Algorithm Steps:
- Select a pivot element.
- Partition the array into two parts.
- Recursively apply the above steps to subarrays.

Time Complexity:
- Worst-case: O(n²) (when pivot divides array poorly)
- Average-case: O(n log n)
- Best-case: O(n log n)

Space Complexity:
- O(log n) (due to recursion stack)

Advantages:
- Generally faster in practice than other O(n log n) algorithms.
- In-place sorting (requires minimal extra space).

Disadvantages:
- Performance depends on pivot selection.
- Not stable by default.

---

Other Sorting Algorithms (Brief Overview)

- Selection Sort: Repeatedly selects the minimum element and places it at the beginning (O(n²)).
- Heap Sort: Uses a binary heap data structure to sort elements (O(n log n)).
- Counting Sort: Efficient for sorting integers within a known range (O(n + k)).
- Radix Sort: Sorts numbers digit by digit (O(d * (n + k))).

---

Summary

Choosing a sorting algorithm depends on:

- Dataset size.
- Data characteristics (e.g., nearly sorted, range of values).
- Memory constraints.
- Stability requirements.

Chapter 9: Searching Algorithms

Introduction

Searching algorithms are used to find an element within a data structure, such as an array or a list. The efficiency of a search algorithm significantly impacts the overall performance of software.

Types of Searching Algorithms

1. Linear Search

Description:
Linear search is the simplest searching algorithm. It sequentially checks each element of the list until it finds the target element or reaches the end of the list.

Algorithm Steps:
- Start from the first element.
- Compare each element with the target.
- If a match is found, return the index.
- If the end is reached without a match, report failure.

Time Complexity:
- Worst-case: O(n)
- Average-case: O(n)
- Best-case: O(1) (if the first element matches)

Space Complexity:
- O(1) (in-place)

Advantages:
- Simple to implement.
- Works on unsorted data.

Disadvantages:
- Inefficient for large datasets.

Applications:
- Small datasets or unsorted data.
- When data structure doesn't support fast random access.

---

2. Binary Search

Description:
Binary search is an efficient algorithm for finding an element in a sorted list. It works by repeatedly dividing the search interval in half.

Algorithm Steps:
- Start with the entire sorted list.
- Find the middle element.
- If the middle element matches the target, return its index.
- If the target is less, repeat on the left sublist.
- If the target is greater, repeat on the right sublist.
- Repeat until the target is found or the sublist is empty.

Time Complexity:
- Worst-case: O(log n)
- Average-case: O(log n)
- Best-case: O(1) (if middle element matches)

Space Complexity:
- O(1) for iterative implementation.
- O(log n) for recursive implementation due to call stack.

Advantages:
- Much faster than linear search on sorted data.
- Efficient for large datasets.

Disadvantages:
- Requires sorted data.
- More complex to implement than linear search.

Applications:
- Searching in sorted arrays or lists.
- Used in many libraries and databases.

---

3. Interpolation Search (Brief Overview)

Description:
Interpolation search improves upon binary search by estimating the position of the target based on the value's distribution.

Time Complexity:
- Average-case: O(log log n) for uniformly distributed data.
- Worst-case: O(n)

Applications:
- Searching in uniformly distributed datasets.

---

4. Exponential Search (Brief Overview)

Description:
Exponential search finds the range where the target might be by repeated doubling, then performs binary search in that range.

Time Complexity:
- O(log n)

Applications:
- Unbounded or infinite lists.

---

Summary

Choosing a searching algorithm depends on:

- Whether data is sorted.
- Dataset size.
- Distribution of data.
- Implementation complexity.

---

Chapter 10: Algorithm Complexity

Introduction

Algorithm complexity is a measure of the amount of resources an algorithm consumes relative to the input size. The most common resources analyzed are time (how long an algorithm takes to run) and space (how much memory it requires).

Big O Notation

Big O notation is used to describe the upper bound of an algorithm’s running time or space requirements in terms of input size n. It focuses on the worst-case scenario and ignores constant factors and lower order terms.

Common Complexity Classes

1. O(1) — Constant Time
- Execution time does not change with input size.
- Example: Accessing an array element by index.

2. O(log n) — Logarithmic Time
- Execution time grows logarithmically with input size.
- Example: Binary search algorithm.

3. O(n) — Linear Time
- Execution time grows linearly with input size.
- Example: Linear search.

4. O(n log n) — Log-Linear Time
- Typical complexity of efficient sorting algorithms like merge sort and quicksort.

5. O(n²) — Quadratic Time
- Execution time grows proportional to the square of input size.
- Example: Bubble sort, insertion sort.

6. O(2^n) — Exponential Time
- Execution time doubles with each additional input element.
- Example: Recursive solutions to the traveling salesman problem.

7. O(n!) — Factorial Time
- Very inefficient, grows factorially with input size.
- Example: Generating all permutations of a set.

Time Complexity vs Space Complexity

- Time Complexity: Amount of time taken to complete as a function of input size.
- Space Complexity: Amount of memory used as a function of input size.

Trade-offs between time and space complexity often arise; optimizing one may increase the other.

Best Case, Average Case, Worst Case

- Best Case: Minimum time taken on any input.
- Average Case: Expected time over all inputs.
- Worst Case: Maximum time taken on any input (used in Big O notation).

Amortized Analysis

- Average time per operation over a sequence of operations.
- Useful when occasional costly operations are offset by many cheap ones (e.g., dynamic array resizing).

Practical Considerations

- Constants and lower order terms can matter in real applications.
- Choosing algorithms depends on data size, hardware, and specific use cases.

Conclusion

Understanding algorithm complexity is essential for writing efficient code and making informed decisions in software development.

Chapter 11: Hash Tables

Definition

A hash table (or hash map) is a data structure that stores key-value pairs and allows for efficient data retrieval. It uses a hash function to compute an index into an array of buckets or slots, from which the desired value can be found.

Hash Function

- Converts a given key into an integer index.
- A good hash function distributes keys uniformly to minimize collisions.
- Should be fast to compute.

Collision Handling

Collisions occur when two keys hash to the same index. Common collision resolution techniques include:

1. Chaining
- Each bucket contains a linked list of entries.
- All elements that hash to the same bucket are stored in the linked list.

2. Open Addressing
- All elements are stored in the array itself.
- On collision, the algorithm probes other slots until an empty one is found.
- Methods include linear probing, quadratic probing, and double hashing.

Operations

1. Insertion
- Compute the hash of the key.
- Insert the key-value pair at the computed index or handle collision.

2. Searching
- Compute the hash of the key.
- Search for the key in the bucket or probe sequence.

3. Deletion
- Locate the key using search.
- Remove the key-value pair.

Performance

- Average-case time complexity for insertion, deletion, and search: O(1).
- Worst-case time complexity: O(n) (when many collisions occur).

Applications

- Implementing databases.
- Caches.
- Symbol tables in compilers.
- Associative arrays.

Advantages

- Provides fast data access.
- Simple and versatile.

Disadvantages

- Requires a good hash function.
- Performance degrades if many collisions occur.
- Uses extra memory for pointers in chaining.

Load Factor

- Ratio of the number of stored elements to the number of buckets.
- High load factors increase the likelihood of collisions.
- Resizing and rehashing help maintain efficient operations.

Conclusion

Hash tables are fundamental data structures that balance speed and memory usage for efficient key-based data retrieval.

Chapter 12: Recursion and Backtracking

Recursion

Definition
Recursion is a programming technique where a function calls itself to solve smaller instances of the same problem until a base case is reached.

Components of Recursion
1. Base Case
- The condition under which the recursion stops.
- Prevents infinite recursion.

2. Recursive Case
- The part where the function calls itself with modified parameters, moving towards the base case.

Example: Factorial Calculation
```python
def factorial(n):
    if n == 0 or n == 1:  # Base case
        return 1
    else:
        return n * factorial(n - 1)  # Recursive case
Advantages of Recursion

Simplifies code for problems that can be broken down into similar subproblems.

Useful in tree and graph traversal, divide and conquer algorithms.

Disadvantages of Recursion

High memory usage due to call stack.

Risk of stack overflow with deep recursion.

Sometimes less efficient than iterative solutions.

Backtracking

Definition
Backtracking is a refinement of recursion that tries to build a solution incrementally and abandons a path ("backtracks") as soon as it determines that this path cannot possibly lead to a valid solution.

Process

Choose: Make a choice from available options.

Explore: Recursively explore the choice.

Un-choose: Undo the choice to try other possibilities.

Example Applications

Solving puzzles like Sudoku.

Finding paths in mazes.

Generating permutations and combinations.

N-Queens problem.

Backtracking Algorithm Structure

If solution is found, return true.

For each option:

Make the choice.

Recur to explore further.

Undo the choice (backtrack).

Advantages

Efficient for constraint satisfaction problems.

Prunes large parts of the search space.

Disadvantages

Can be slow if pruning is insufficient.

Requires careful implementation.

Optimization Techniques

Use heuristics to decide order of choices.

Apply memoization to avoid repeated work.

Implement pruning to cut off unpromising branches early.

Conclusion
Recursion and backtracking are powerful techniques for solving complex problems with elegant solutions, especially in combinatorial and search problems.